#!/usr/bin/env python3
"""
WEB RESEARCH ENGINE - –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–æ–ª—å–∫–æ –ë–ï–°–ü–õ–ê–¢–ù–´–ï API —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏!
"""

import os
import json
import time
import asyncio
import aiohttp
import requests
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import re
from urllib.parse import quote, urlparse

@dataclass
class ResearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è"""
    query: str
    sources: List[Dict[str, Any]]
    analysis: str
    confidence: float
    timestamp: datetime
    total_sources: int

class WebResearchEngine:
    """üöÄ –†–µ–≤–æ–ª—é—Ü–∏–æ–Ω–Ω—ã–π –¥–≤–∏–∂–æ–∫ –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""

    def __init__(self):
        # –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ API —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –ª–∏–º–∏—Ç–∞–º–∏
        self.apis = {
            'serpapi': {
                'url': 'https://serpapi.com/search',
                'key': os.getenv('SERPAPI_KEY', ''),
                'limit': 100,  # 100/–º–µ—Å—è—Ü –±–µ—Å–ø–ª–∞—Ç–Ω–æ
                'enabled': bool(os.getenv('SERPAPI_KEY'))
            },
            'jina_reader': {
                'url': 'https://r.jina.ai/',
                'limit': 1000,  # 1000/–¥–µ–Ω—å –±–µ—Å–ø–ª–∞—Ç–Ω–æ
                'enabled': True  # –ù–µ —Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á–∞!
            },
            'brave_search': {
                'url': 'https://api.search.brave.com/res/v1/web/search',
                'key': os.getenv('BRAVE_API_KEY', ''),
                'limit': 2000,  # 2000/–º–µ—Å—è—Ü –±–µ—Å–ø–ª–∞—Ç–Ω–æ
                'enabled': bool(os.getenv('BRAVE_API_KEY'))
            },
            'webscraping_ai': {
                'url': 'https://api.webscraping.ai/html',
                'key': os.getenv('WEBSCRAPING_AI_KEY', ''),
                'limit': 1000,  # 1000/–º–µ—Å—è—Ü –±–µ—Å–ø–ª–∞—Ç–Ω–æ
                'enabled': bool(os.getenv('WEBSCRAPING_AI_KEY'))
            }
        }

        self.usage_stats = {
            'serpapi': 0,
            'jina_reader': 0,
            'brave_search': 0,
            'webscraping_ai': 0
        }

        print("üîç WebResearchEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω:")
        for api, config in self.apis.items():
            status = "‚úÖ" if config['enabled'] else "‚ùå"
            print(f"   {api}: {status} (–ª–∏–º–∏—Ç: {config['limit']})")

    async def research_billionaire_interviews(self, query: str, count: int = 20) -> ResearchResult:
        """üéØ –ò—Å—Å–ª–µ–¥—É–µ—Ç –∏–Ω—Ç–µ—Ä–≤—å—é –º–∏–ª–ª–∏–∞—Ä–¥–µ—Ä–æ–≤"""

        # –†–∞—Å—à–∏—Ä—è–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ç–µ—Ä–≤—å—é
        enhanced_queries = [
            f"{query} interview billionaire rich successful entrepreneur",
            f"{query} –∏–Ω—Ç–µ—Ä–≤—å—é –º–∏–ª–ª–∏–∞—Ä–¥–µ—Ä –±–æ–≥–∞—Ç—ã–π –±–∏–∑–Ω–µ—Å–º–µ–Ω",
            f"{query} podcast CEO founder startup unicorn",
            f"{query} TED talk business leader motivation"
        ]

        all_sources = []

        for enhanced_query in enhanced_queries:
            print(f"üîç –ò—â—É: {enhanced_query}")

            # –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –¥–æ—Å—Ç—É–ø–Ω—ã–µ API
            sources = await self._multi_search(enhanced_query, count // len(enhanced_queries))
            all_sources.extend(sources)

            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            await asyncio.sleep(1)

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏—Ä—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        unique_sources = self._deduplicate_sources(all_sources)

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã —á–µ—Ä–µ–∑ Groq
        analysis = await self._analyze_interview_content(unique_sources[:count], query)

        return ResearchResult(
            query=query,
            sources=unique_sources[:count],
            analysis=analysis,
            confidence=min(len(unique_sources) / count, 1.0),
            timestamp=datetime.now(),
            total_sources=len(unique_sources)
        )

    async def _multi_search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """üîÑ –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ API –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""

        search_tasks = []

        # SerpAPI (Google)
        if self.apis['serpapi']['enabled'] and self.usage_stats['serpapi'] < self.apis['serpapi']['limit']:
            search_tasks.append(self._search_serpapi(query, limit))

        # Brave Search
        if self.apis['brave_search']['enabled'] and self.usage_stats['brave_search'] < self.apis['brave_search']['limit']:
            search_tasks.append(self._search_brave(query, limit))

        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        if search_tasks:
            results = await asyncio.gather(*search_tasks, return_exceptions=True)

            all_results = []
            for result in results:
                if isinstance(result, list):
                    all_results.extend(result)

            return all_results

        return []

    async def _search_serpapi(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """üåê –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ SerpAPI (Google)"""
        try:
            params = {
                'q': query,
                'api_key': self.apis['serpapi']['key'],
                'engine': 'google',
                'num': min(limit, 10),
                'gl': 'us',
                'hl': 'en'
            }

            async with aiohttp.ClientSession() as session:
                async with session.get(self.apis['serpapi']['url'], params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.usage_stats['serpapi'] += 1

                        results = []
                        for item in data.get('organic_results', []):
                            results.append({
                                'title': item.get('title', ''),
                                'url': item.get('link', ''),
                                'snippet': item.get('snippet', ''),
                                'source': 'google',
                                'date': item.get('date', ''),
                                'type': 'web'
                            })

                        return results

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ SerpAPI: {e}")

        return []

    async def _search_brave(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """ü¶Å –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ Brave Search"""
        try:
            headers = {
                'X-Subscription-Token': self.apis['brave_search']['key']
            }

            params = {
                'q': query,
                'count': min(limit, 20),
                'offset': 0,
                'country': 'us'
            }

            async with aiohttp.ClientSession() as session:
                async with session.get(self.apis['brave_search']['url'],
                                     params=params, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        self.usage_stats['brave_search'] += 1

                        results = []
                        for item in data.get('web', {}).get('results', []):
                            results.append({
                                'title': item.get('title', ''),
                                'url': item.get('url', ''),
                                'snippet': item.get('description', ''),
                                'source': 'brave',
                                'date': item.get('published', ''),
                                'type': 'web'
                            })

                        return results

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Brave Search: {e}")

        return []

    async def extract_content_from_url(self, url: str) -> str:
        """üìÑ –ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ URL —á–µ—Ä–µ–∑ Jina Reader (–ë–ï–°–ü–õ–ê–¢–ù–û!)"""
        try:
            if self.usage_stats['jina_reader'] >= self.apis['jina_reader']['limit']:
                return "–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç Jina Reader"

            # Jina Reader - –ë–ï–°–ü–õ–ê–¢–ù–´–ô —Å–µ—Ä–≤–∏—Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ URL –≤ —Ç–µ–∫—Å—Ç
            reader_url = f"{self.apis['jina_reader']['url']}{url}"

            async with aiohttp.ClientSession() as session:
                async with session.get(reader_url, timeout=30) as response:
                    if response.status == 200:
                        content = await response.text()
                        self.usage_stats['jina_reader'] += 1
                        return content[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}")

        return ""

    def get_youtube_transcript(self, youtube_url: str) -> str:
        """üì∫ –ë–ï–°–ü–õ–ê–¢–ù–û–ï –ø–æ–ª—É—á–µ–Ω–∏–µ —Å—É–±—Ç–∏—Ç—Ä–æ–≤ YouTube"""
        try:
            from youtube_transcript_api import YouTubeTranscriptApi

            # –ò–∑–≤–ª–µ–∫–∞–µ–º video_id –∏–∑ URL
            video_id = self._extract_youtube_id(youtube_url)
            if not video_id:
                return ""

            # –ü–æ–ª—É—á–∞–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã (–ë–ï–°–ü–õ–ê–¢–ù–û, –±–µ–∑ –ª–∏–º–∏—Ç–æ–≤!)
            transcript = YouTubeTranscriptApi.get_transcript(video_id, languages=['en', 'ru'])

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤ —Ç–µ–∫—Å—Ç
            full_text = ' '.join([entry['text'] for entry in transcript])
            return full_text[:10000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—É–±—Ç–∏—Ç—Ä–æ–≤: {e}")
            return ""

    def _extract_youtube_id(self, url: str) -> Optional[str]:
        """üé¨ –ò–∑–≤–ª–µ–∫–∞–µ—Ç ID –≤–∏–¥–µ–æ –∏–∑ YouTube URL"""
        patterns = [
            r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})',
            r'youtube\.com\/watch.*[&?]v=([a-zA-Z0-9_-]{11})'
        ]

        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)

        return None

    async def _analyze_interview_content(self, sources: List[Dict], query: str) -> str:
        """üß† –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –∏–Ω—Ç–µ—Ä–≤—å—é —á–µ—Ä–µ–∑ Groq"""

        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        content_snippets = []
        for source in sources[:10]:  # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–æ 10 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
            snippet = source.get('snippet', '')
            if 'youtube.com' in source.get('url', ''):
                # –î–ª—è YouTube –ø–æ–ª—É—á–∞–µ–º —Å—É–±—Ç–∏—Ç—Ä—ã
                transcript = self.get_youtube_transcript(source['url'])
                if transcript:
                    snippet = transcript[:1000]

            content_snippets.append({
                'title': source.get('title', ''),
                'content': snippet,
                'url': source.get('url', '')
            })

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Groq –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        from smart_ai_generator import SmartAIGenerator
        ai_generator = SmartAIGenerator()

        analysis_prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–∏ –∏–Ω—Ç–µ—Ä–≤—å—é –∏ –º–∞—Ç–µ—Ä–∏–∞–ª—ã —É—Å–ø–µ—à–Ω—ã—Ö –ª—é–¥–µ–π –ø–æ —Ç–µ–º–µ: {query}

–ò–°–¢–û–ß–ù–ò–ö–ò:
{json.dumps(content_snippets, ensure_ascii=False, indent=2)}

–°–æ–∑–¥–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑:
1. –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
2. –û–±—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —É—Å–ø–µ—Ö–∞
3. –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã
4. –¶–∏—Ç–∞—Ç—ã –∏ –∏–Ω—Å–∞–π—Ç—ã
5. –ü—Ä–∏–º–µ–Ω–∏–º—ã–µ —É—Ä–æ–∫–∏

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: –ø–æ–¥—Ä–æ–±–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
        """

        try:
            analysis = ai_generator._call_groq_api(analysis_prompt, 'llama-3.1-8b-instant')
            return analysis
        except Exception as e:
            return f"–ù–∞–π–¥–µ–Ω–æ {len(sources)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ —Ç–µ–º–µ '{query}'. –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}"

    def _deduplicate_sources(self, sources: List[Dict]) -> List[Dict]:
        """üîÑ –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"""
        seen_urls = set()
        unique_sources = []

        for source in sources:
            url = source.get('url', '')
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_sources.append(source)

        return unique_sources

    def get_usage_stats(self) -> Dict[str, Any]:
        """üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è API"""
        return {
            'usage': self.usage_stats,
            'limits': {api: config['limit'] for api, config in self.apis.items()},
            'remaining': {
                api: max(0, config['limit'] - self.usage_stats[api])
                for api, config in self.apis.items()
            }
        }

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
async def test_research_engine():
    """üß™ –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –¥–≤–∏–∂–æ–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π"""
    engine = WebResearchEngine()

    print("üöÄ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –∏–Ω—Ç–µ—Ä–≤—å—é –º–∏–ª–ª–∏–∞—Ä–¥–µ—Ä–æ–≤...")

    result = await engine.research_billionaire_interviews(
        "–±–∏–∑–Ω–µ—Å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π",
        count=10
    )

    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {result.total_sources}")
    print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result.confidence:.2%}")
    print(f"üìù –ê–Ω–∞–ª–∏–∑: {result.analysis[:200]}...")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    stats = engine.get_usage_stats()
    print(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ API: {stats['usage']}")

if __name__ == "__main__":
    asyncio.run(test_research_engine())